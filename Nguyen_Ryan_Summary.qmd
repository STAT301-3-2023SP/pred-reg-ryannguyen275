---
title: "Regression Prediction Executive Summary"
author: "Ryan Nguyen"

format:
  html:
    toc: true
    embed-resources: true
    echo: false
    link-external-newwindow: true
    
execute:
  warning: false

from: markdown+emoji  
---

# Predicting Y (Regression)

**GitHub Repo:** <https://github.com/STAT301-3-2023SP/pred-reg-ryannguyen275>

To begin the prediction problem, the data was read in and explored. The data was split into a training data set and test data set with a 0.75 proportion, stratified on the outcome variable y, in order to prevent overfitting. From here, the data was explored to see the missingness of all variables, the distribution of the outcome variable, variables with zero variance, and miscoded categorical variables, to look for potential transformations. The performance metric that will be used is rmse.

After exploring the data, the next step was variable selection. Since there were over 750 variables, it was important to find which variables were impactful in predicting the outcome variable. An initial recipe was created and tuned with a lasso model and a random forest model to do so. In the lasso model, variables with a coefficient of 0 were filtered out. In the random forest model, an importance plot was constructed to view the important variable. This process was repeated multiple times with minor changes in the initial recipe. Ultimately, the lasso model resulted in 15 variables, and the random forest models resulted in 16 unique variables.

From here, feature engineering could occur. Throughout all attempts, the training data was folded using v-fold cross-validation, with 5 folds and 3 repeats, to help prevent overfitting.

### Attempt 1

For Attempt 1, the feature engineering was focused on finding the best recipe and variable selection. Thus, multiple recipes were constructed-- some using just the lasso variables, some using just the random forest variables, and some using a combination of both. While the recipes varied, each included `step_nzv()` to remove zero variance variables, `step_normalize()` to center and normalize all variables, and `step_impute_()` to impute missing variables in different ways. The recipes were prepped and baked to ensure there were no computational errors. 8 models were run and tuned with the various recipes, including a boosted tree model, an elastic net model, a k-nearest neighbors model, a MARS model, a neural network model, a random forest model, a SVM polynomial model, and an SVM radial model. Each model was submitted on Kaggle. The recipe that resulted in the lowest RMSE was the recipe that used a combination of ALL variables from lasso variable selection and random forest selection, with 25 variables being used in total.

### Attempt 2

In Attempt 2, the goal was to find which specific models were performing the best for this specific problem. The recipe consisted of `step_nzv()`, `step_normalize()`, and `step_impute_knn()`. All 8 models were each tuned with their respective parameters, using the saved folds and recipe, and the workflows and tuning results were saved. The model with the lowest rmse was selected from each model, and the results were compared in the table and graph below.

```{r}
library(tidyverse)
library(tidymodels)
library(kableExtra)
load("attempt_2/results/model_results.rda")

mr_2 <- model_results %>% 
  select(wflow_id, mean, std_err) %>% 
  rename(model = wflow_id,
         rmse = mean)

mr_2_table <- mr_2 %>% 
  kbl()  %>%
  kable_styling()

mr_2_table

ggplot(mr_2, aes (x = model, y = rmse, color = model)) +
  geom_point() +
  labs(y = "rmse", x = "Model") +
  geom_errorbar(aes(ymin = rmse - std_err, ymax = rmse + std_err), width = 0.3) +
  ggtitle(label = "Attempt 2 Best Results") +
  ylim(c(9.35, 10.75)) + 
  theme(legend.position = "none")
```

Here, the best model was SVM radial with an rmse of 9.778, followed by SVM poly with an rmse of 9.856, then random forest with an rmse of 9.860. Since SVM radial was the best, the workflow was finalized and fit on the entire training set; this fit was used to predict the final testing set. The predictions were then written as a .csv file and submitted, resulting in an rmse of 9.17026, which does not pass the benchmark of 9.0. This process was repeated with SVM poly and the random forest model; however none of these models passed the benchmark.

### Attempt 3

For Attempt 3, the goal was to stack the best 3 models that was found and edit tuning parameters in order to improve and lower the RMSE. The recipe was the same as attempt 2. The random forest model, the SVM poly model, and SVM radial model were rerun, this time ensuring to save workflow and predictions with `control_stack_resamples()`. The models were tuned and combined into a data stack, then regularization was performed to determine how member model output will ultimately be combined in the final prediction by fitting a lasso model on the stack, predicting the true assessment set outcome using the predictions from each of the candidate members. Then, the candidates with non-zero stacking coefficients were fit on the full training set, which was used to predict the final testing set. The predictions were then written as a .csv file and submitted, resulting in an rmse of 8.926, which passes the benchmark of 9.0. Thus, stacking the models improved the overall prediction significantly.

While the project could end here, it would be interesting to see how stacking various other models would impact the performance.

### Attempt 4

Although the benchmark has been passed, for Attempt 4, methods are explored to further improve the model. To edit the recipe, a `step_YeoJohnson()` was added on all predictors to see if this transformation helped, as well as a `step_corr()` to remove directly correlated variables. This recipe was run and tuned on all 8 models, with the intention of stacking. Initially, the best 6 performing models from Attempt 2 were stacked, fitted, and used to predict on the final testing set including the 3 from Attempt 3 plus neural networks, boosted tree, and elastic net. Further, all 8 models were stacked to see if this would lower the rmse as well. The coefficients from the models can be seen in the graphs below. Evidently, both models only kept the boosted tree, linear regression, neural networks, SVM poly, and SVM radial models.

```{r}
library(stacks)

```

![](attempt_4/results/stacks6.png){width="325"} ![](attempt_4/results/stacks8.png){width="325"}

This stacked models were used to predict the final testing set. The predictions were then written as a .csv file and submitted for both stacked ensemble models. The model with the top 6 (later 5) models resulted in an rmse of 8.81411, which is an improvement from the model with the top 3 stacked. The model with all 8 (later 5) models resulted in an rmse of 8.91895, which is better than the model with the top 3 stacked, but not better than the model with the top 6 stacked.

Therefore, the best performing model was the stacked ensemble model with random forest, boosted tree, linear regression, neural networks, SVM poly, and SVM radial models, with a recipe that included `step_YeoJohnson()` and `step_corr()` .

Finally, this problem used a variety of feature engineering and machine learning to predict y. While the rmse met the benchmark, it could be improved. In the future, to improve this prediction model, the tuning parameters could be further adjusted to better the models, the recipe could be changed (varying imputation methods, adding transformations, adding interactions, etc.), or the variety/combination of models that could be stacked in an ensemble model could be altered. All of these could help us achieve a lower rmse and improve our prediction of y.
